# 리눅스 실무에서 사용하는 유용한 명령어 모음 정리

IT 회사에서 개발 및 운영을 하면 요즘같이 OpenSearch 같은 GUI 기반으로 로그를 검색해서 찾아보지만.. 불가피하게 서버에 접속하여 로그를 까보는 경우도 생기게 됩니다.

로그 파일에서 원하는 내용을 찾기 위한 간단한 명령어는 체득이되어서 쓸 수 있지만 실무에서 더 빠르고 유용한 리눅스 명령어를 쉽게 볼 수 있도록 정리하였습니다.


## 로그 파일에서 특정 문자열 검색

보통 grep 명령어를 사용해서 썼지만 가끔 내가 로깅을 대소문자로 남겼는지 기억이 안나는 경우에는 이 명령어가 유용한것 같습니다.


- 지정한 디함토리 경로의 하위까지 해당 텍스트를 검색하지만, 대소문자는 무시함

```
grep -r -i "plain text" ./test
```

- 정규식을 적용하여 or 연산자로 검색

```
grep -r -E "(plain text|PLAIN TEXT)" ./test
```

-v 옵션은 특정 문자열이 들어있지 않는 라인을 제외하고 출력합니다.

```
grep -v "c" test.txt
```

## 파일 내용 검색

보통은 vim 노멀모드에서 검색모드(`/`)로 전환 후 찾고자 하는 내용을 입력 한 후에 
`N`키를 눌러서 파일 내용을 검색 할 수 있습니다. 

- 역방향 검색
    - 만약 역방향으로 다시 돌아가려면 `Shift + N`을 누르면 됩니다.

- 정규표현식을 적용하여 검색

```
/\v (world|file)
```

## top 명령어

top 명령어는 시스템 상태를 조사해서 그 결과를 표시합니다. 마찬가지로 CPU를 사용합니다.

Load Average는 평균 시간동안 CPU가 처리하는 걸 기다리는 작업 개수를 의미합니다.

실제로 mac 환경에서 top 명령어를 실행하면 `Load Average`가 3개가 나오는데 각각 1분, 5분, 15분 기준으로 CPU를 기다리는 작업 수 입니다.

![image](https://user-images.githubusercontent.com/22395934/221343414-03095500-237f-48e8-9b8e-31502f160f0e.png)

제가 사용하고 있는 Mac(Intel)은 6코어이고, 1분당 평균 작업대기량은 3.71로 이정도면 CPU 상태가 충분히 원활하다고 볼 수 있습니다.

만약 `Load Average`가 CPU 코어수보다 크거나 같으면 과부하 상태로 판단할 수 있습니다.

또한 CPU 사용률과 CPU 시간 양쪽이 큰 프로세스는 과부하의 원인의 가능성이 높기 때문에 `TIME`과 `CMD` 필드를 보고 과부하의 원인을 파악하는데 도움을 줄 수 있습니다.

`Load Average`는 CPU 사용률 뿐만 아니라 디스크 I/O로 인해서 메모리가 부족해지는 경우에도 높아 질 수 있습니다. 

따라서 top 명령어로 `MEM` 필드를 보고 메모리를 많이 사용하고 있는 프로세스를 보고 Load Average를 높이는 프로세스인지 의심하는 습관을 기르는게 좋습니다.

> top 명령어는 기본적으로 CPU 사용량 순서로 보여주지만 만약 메모리 사용량 순서로 보고 싶으면 `Shift + M` 키를 누르면 됩니다.

## 일괄 치환

가끔 쉘 스크립트를 작성할 때 변수명을 잘못 사용해서 바꾸는 경우가 있는데.. 이때 너무 많이 선언되어 있으면 노가다성으로 다 바꿔줘야되는데 리눅스는 일괄치환 명령어를 제공해주기 때문에 쉽게 사용할 수 있습니다.

```
:%s/원본/수정문
```

normal 모드에서 위와 같은 키워드로 일괄 치환이 가능합니다.

## cut 명령어

로그를 볼 때 보고 싶은 특정 관심사들이 있습니다. 예를 들어 서비스 명, 일자시간, API Path 등이 대표적입니다.

`grep`을 사용해서 원하는 문자열을 검색할 수 있지만 파이프 라인으로 넘어온 내용의 각 줄마다 필요한 부분만 자를때 `cut` 명령어를 사용하면 편합니다.

```
cat access.log | grep -v "/live" |cut -d " " -f 7
```


- -d(delimiter): 구분자
- -f(fields): 추출할 위치

위 명령어는 /live가 들어간 라인은 제외하고, 공백을 구분자(delimiter)로 7번째 위치만 잘라서 해당 로그 파일을 열겠다는 명령어 입니다.

cut은 단순히 일부 열만 추출하는게 아니고, 그 반대인 일부 열만 제거하는것도 가능합니다.

주로 csv 파일을 작업할때 유용합니다.

예를 들어서 특정 1,3,4,6번째만 출력하고 싶다면
아래와 같이 명령어를 사용하면 됩니다.

#### 특정 열만 출력하고 나머지 열은 버리는 경우

```
cut -d "," -f 1,3,4,6
```

#### 연속한 열만 출력하는 경우

```
cut -d "," -f 1-3 
```
1,2,3번째 열만 출력합니다.

만약 어떤 열 이후의 모든 열을 저장하고 싶다면 끝을 생략하면 됩니다. `시작-`


## count 집계 명령어

앞에서 배운 cut 명령어와 파이프 라인으로 연결하여 집계명령어도 사용가능합니다.

먼저, `sort`, `unique` 명령어를 알아야 됩니다.

- sort: 알파벳 순으로 정렬
- unique: 중복 제거

```
cat /var/log/access.log | grep -v "/live" | cut -d " " -f 7 | sort | unique -c | less 
```

unique는 같은 내용이 몇번 등장했는가를 셀 수있습니다. -c 옵션을 주면 중복횟수를 출력이 가능합니다.

여기서 포인트는 중복제거하기 전에 반드시 정렬부터 해줘야 원하는 집계 결과가 나온다는 것입니다. sort와 unique는 반드시 함께 써야 됩니다.

> 정렬(sort)을 내림차순으로 보기 위해서는 -r 옵션을 주면 됩니다.

```
cat /var/log/access.log | grep -v "/live" | cut -d " " -f 7 | sort | unique -c | sort -r 
```

### tail과 head

만약 특정 API Path의 상위 10건과 하위 10건을 집계하고 싶으면 `head`, `tail` 명령어를 위의 `sort`, `unique` 명령어와 파이프 라인으로 조합하면 쉽게 추출할 수 있습니다.


#### 상위 10개 API Path 건수 집계

```
cat /var/log/access.log | grep -v "/live" | cut -d " " -f 7 | sort | unique -c | sort -r | head
```

#### 하위 10개 API Path 건수 집계

```
cat /var/log/access.log | grep -v "/live" | cut -d " " -f 7 | sort | unique -c | sort -r | tail
```

10 라인이아니고 특정 라인을 지정하고 싶으면 아래와 같은 옵션을 추가하면 됩니다.

- head -n 3: 앞에서부터 3줄을 추출

- tail -n 20: 끝에서 20줄을 추출

> 만약 필요없는 줄을 제거하고 싶으면 tail 명령어의 경우 -n 옵션은 플러스 기호를 써서 tail -n +6을 주게되면 6번째 이후로 출력한다는 의미입니다. head의 경우에는 -n 옵션에 마이너스 기호를 써서 head -n -5를 입력하면 마지막 5줄을 제외한 나머지 모든 줄을 출력해줍니다.


## csv 파일처리를 위한 명령어

보통 로그파일을 csv로 추출하게 되는 경우 아래와 같은 명령어들도 유용합니다.

```
cat items.csv | cut -d "," -f 1-3 | sort -t "," -k 2 -b
```

위 옵션은 1,2,3번째 열만 출력하고, 2번째 열을 기준으로 정렬하겠다는 명령어입니다.

-t는 --field-seperator를 의미하는 구분자입니다. 

-k 옵션은 key를 의미하는 열번호를 지정할때 사용하는 옵션입니다.

-b 옵션은 --ignore-leading-blanks를 뜻하며 오른쪽 줄맞춤을 위해서 넣은 스페이스를 무시하고 문자열을 정렬하기 위해 사용합니다.

> csv는 기본적으로 ,(컴마)를 구분자로 사용하기 때문에 ,로 구분자를 지정하였습니다.

최종적으로 csv 파일을 특정 열로 정렬하고 새로운 파일로 저장하고 싶다면 리다이렉트를 사용하면 됩니다.

```
cat items.csv | grep -v "/live" | cut -d "," -f 1-3 | sort -t "," -k 3 -n > ./items-sorted.csv
```

여기서 -n을 사용하는 이유는 sort는 단순정렬 시 라인의 첫번째 문자열부터 비교하기 때문에 의도치 않는 정렬 결과가 나올 수 있기 때문에 정렬 기준이 되는 필드가 숫자라면 -n(number)를 지정해줘야 합니다.

위에서 unique와 함께 파이프라인에서 사용했을 때 sort에 -n 옵션을 주지 않았던 이유는 unique -c 명령어 결과가 숫자를 오른쪽 정렬을 하기 때문입니다. 


\>, \>> 의차이가 햇갈려서 다시 정리하였습니다.

```
\>: 파일이 있으면 지우고 새로운 파일을 만듭니다
\>>: 기존 파일에 추가합니다. 파일명이 같으면 덮어쓰기 때문에 반드시 다른 이름을 지정해야합니다.
```
